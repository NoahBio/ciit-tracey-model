# Omniscient RL Training Configuration
# Agent has full knowledge of client internal dynamics (u_matrix, RS, bond, etc.)

experiment_name: "omniscient_rl_cold_stuck"

# Environment Configuration
patterns: ["cold_stuck"]
mechanism: "frequency_amplifier"
threshold: 0.9  # Success threshold (90th percentile)
max_sessions: 100
enable_perception: true  # Enable perception for more realistic simulation
baseline_accuracy: 0.2  # 20% baseline perception accuracy

# Omniscient-Specific
use_omniscient_wrapper: true  # Enable omniscient observation wrapper

# RL Algorithm Parameters
# Adjusted for richer observation space (471 dims vs 417 dims)
total_timesteps: 200_000  # Reduced (should learn faster with perfect information)
learning_rate: 0.0001  # Lower LR for more complex input space
n_envs: 8  # Parallel training environments
batch_size: 128  # Larger batch for more complex features
n_epochs: 10  # SGD epochs per batch

# PPO-specific
gamma: 0.99  # Discount factor
gae_lambda: 0.95  # GAE parameter
clip_range: 0.2  # PPO clip epsilon
ent_coef: 0.01  # Entropy bonus coefficient
vf_coef: 0.5  # Value function loss coefficient
max_grad_norm: 0.5  # Gradient clipping

# Network Architecture
hidden_size: 256  # Hidden layer size (same as model-free for comparison)

# Logging and Evaluation
log_dir: "logs/omniscient_rl"
save_freq: 10_000  # Save checkpoint every N timesteps
eval_freq: 5_000  # Evaluate every N timesteps
eval_episodes: 10  # Episodes per evaluation

# Training Infrastructure
seed: 42  # Random seed for reproducibility
